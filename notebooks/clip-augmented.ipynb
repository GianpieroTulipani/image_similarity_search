{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:14:45.336600Z",
     "iopub.status.busy": "2024-12-12T17:14:45.336230Z",
     "iopub.status.idle": "2024-12-12T17:14:45.349759Z",
     "shell.execute_reply": "2024-12-12T17:14:45.348828Z",
     "shell.execute_reply.started": "2024-12-12T17:14:45.336567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict\n",
    "import torch\n",
    "import os\n",
    "\n",
    "transform = { \n",
    "    \"base\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ]),\n",
    "\n",
    "    \"crop\": transforms.Compose([\n",
    "        transforms.CenterCrop((266, 375)),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ]),\n",
    "\n",
    "    \"brightness\": transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.3),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ])\n",
    "}\n",
    "\n",
    "def augmented_dataset(path, mode):\n",
    "    base_dataset = get_dataset(path, mode, augmentation_type= \"base\" )\n",
    "    crop_dataset = get_dataset(path, mode, augmentation_type=\"crop\")\n",
    "    brightness_dataset = get_dataset(path, mode, augmentation_type=\"brightness\")\n",
    "\n",
    "    return ConcatDataset([base_dataset, crop_dataset, brightness_dataset])\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, transform: Optional[Dict[str, transforms.Compose]] = None, augmentation_type: str = None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.augmentation_type = augmentation_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        labels = self.data.iloc[idx]\n",
    "        path = labels[\"IMAGE_PATH\"]\n",
    "        description = labels[\"DESCRIPTION\"]\n",
    "        image_id = labels[\"MFC\"]\n",
    "\n",
    "        assert os.path.exists(path), f\"Image {path} does not exist\"\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        description = clip.tokenize(description, truncate=True)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform[self.augmentation_type](image)\n",
    "        return image, description, (self.augmentation_type, image_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        images, description, images_id = zip(*batch)\n",
    "        return torch.stack(images), torch.vstack(description), list(images_id)\n",
    "\n",
    "\n",
    "def get_dataset(path: str, df_path: str, augmentation_type: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Returns a DataLoader object for the dataset at the specified path.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(df_path)\n",
    "    path = Path(path) if isinstance(path, str) else path\n",
    "    df[\"IMAGE_PATH\"] = df[\"IMAGE_PATH\"].apply(lambda x: path / x)\n",
    "    return ImageDataset(data=df, augmentation_type=augmentation_type, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:21:58.585846Z",
     "iopub.status.busy": "2024-12-12T17:21:58.585148Z",
     "iopub.status.idle": "2024-12-12T17:21:59.402567Z",
     "shell.execute_reply": "2024-12-12T17:21:59.401665Z",
     "shell.execute_reply.started": "2024-12-12T17:21:58.585806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ecommerce_path=\"/kaggle/input/armani-catalogue/images/images\"\n",
    "train_csv=\"/kaggle/input/armani-traintest/train.csv\"\n",
    "test_csv= \"/kaggle/input/armani-traintest/test.csv\"\n",
    "dataset = augmented_dataset(ecommerce_path,train_csv)\n",
    "test_dataset= get_dataset(ecommerce_path,test_csv, \"base\")\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=os.cpu_count(), collate_fn=ImageDataset.collate_fn)\n",
    "test_dataloader= DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=os.cpu_count(), collate_fn=ImageDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:18:47.966392Z",
     "iopub.status.busy": "2024-12-12T17:18:47.965759Z",
     "iopub.status.idle": "2024-12-12T17:18:52.511630Z",
     "shell.execute_reply": "2024-12-12T17:18:52.510650Z",
     "shell.execute_reply.started": "2024-12-12T17:18:47.966357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch import nn\n",
    "import clip\n",
    "\n",
    "\n",
    "\n",
    "# Load the model and processor\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:18:54.754546Z",
     "iopub.status.busy": "2024-12-12T17:18:54.754216Z",
     "iopub.status.idle": "2024-12-12T17:18:54.760708Z",
     "shell.execute_reply": "2024-12-12T17:18:54.759699Z",
     "shell.execute_reply.started": "2024-12-12T17:18:54.754516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:22:01.676135Z",
     "iopub.status.busy": "2024-12-12T17:22:01.675392Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2375eab810a4e27a236c0595c530598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20,Train Loss: 1.3992\n",
      "Epoch 1/20, Val Loss: 1.0607\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033f2d1c099c4069a698457c0bfb08d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20,Train Loss: 0.6878\n",
      "Epoch 2/20, Val Loss: 0.9388\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23823389abb141c7b0999e0ff3ea04fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20,Train Loss: 0.4530\n",
      "Epoch 3/20, Val Loss: 0.9179\n",
      "Validation loss improved. Model saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d215571dce4ddaace4754f397e2c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6,betas=(0.9,0.98),eps=1e-6,weight_decay=0.4) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "#define the number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# Early Stopping\n",
    "patience = 5  # Numero massimo di epoche senza miglioramenti\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improvement = 0\n",
    "\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    #plotting the progress bar  \n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    #iterate over the training dataloader\n",
    "    train_loss=0\n",
    "    val_loss=0\n",
    "    for batch in pbar:\n",
    "         #set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        #load the images and texts\n",
    "        images,texts, _ = batch \n",
    "\n",
    "        \n",
    "        images= images.to(device)\n",
    "\n",
    "        texts = texts.to(device)\n",
    "    \n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        \n",
    "        train_loss+=total_loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss/len(train_dataloader)\n",
    "    tqdm.write(f\"Epoch {epoch+1}/{num_epochs},Train Loss: {avg_train_loss:.4f}\")\n",
    "    #validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #iterate over the test dataloader\n",
    "        for batch in test_dataloader:\n",
    "            images, texts, _ = batch\n",
    "            images= images.to(device)\n",
    "            texts = texts.to(device)\n",
    "        \n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            \n",
    "            # Compute loss\n",
    "            ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            val_loss+=total_loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss/len(test_dataloader)\n",
    "    tqdm.write(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "     # Check Early Stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        no_improvement = 0\n",
    "        # Salva il modello migliore\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"/kaggle/working/clipmodel_5.pt\") #just change to your preferred folder/filename\n",
    "        print(\"Validation loss improved. Model saved.\")\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        print(f\"No improvement for {no_improvement} epoch(s).\")\n",
    "\n",
    "    if no_improvement >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "        \n",
    "\n",
    "    #update the progress bar\n",
    "    pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5919931,
     "sourceId": 9738631,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6290365,
     "sourceId": 10182828,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
